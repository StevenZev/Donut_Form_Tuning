{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85894ffd",
   "metadata": {},
   "source": [
    "### CPU Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57e28d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForDonut:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: bool = True\n",
    "    max_length: int = None\n",
    "    pad_to_multiple_of: int = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        # Split inputs and labels since they have different lengths\n",
    "        pixel_values = [feature[\"pixel_values\"] for feature in features]\n",
    "        decoder_input_ids = [feature[\"decoder_input_ids\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        \n",
    "        # Stack pixel values\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        \n",
    "        # Pad decoder input ids and labels to the same length\n",
    "        max_length = max(len(seq) for seq in decoder_input_ids + labels)\n",
    "        if self.max_length is not None:\n",
    "            max_length = min(max_length, self.max_length)\n",
    "        \n",
    "        # Pad decoder input ids\n",
    "        decoder_input_ids_padded = []\n",
    "        for seq in decoder_input_ids:\n",
    "            if len(seq) < max_length:\n",
    "                padding = [self.tokenizer.pad_token_id] * (max_length - len(seq))\n",
    "                padded_seq = torch.cat([seq, torch.tensor(padding)])\n",
    "            else:\n",
    "                padded_seq = seq[:max_length]\n",
    "            decoder_input_ids_padded.append(padded_seq)\n",
    "        \n",
    "        # Pad labels\n",
    "        labels_padded = []\n",
    "        for seq in labels:\n",
    "            if len(seq) < max_length:\n",
    "                padding = [-100] * (max_length - len(seq))  # Use -100 for padding in labels\n",
    "                padded_seq = torch.cat([seq, torch.tensor(padding)])\n",
    "            else:\n",
    "                padded_seq = seq[:max_length]\n",
    "            labels_padded.append(padded_seq)\n",
    "        \n",
    "        # Stack tensors\n",
    "        decoder_input_ids = torch.stack(decoder_input_ids_padded)\n",
    "        labels = torch.stack(labels_padded)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "class InsuranceClaimDataset(Dataset):\n",
    "    \"\"\"Dataset class for insurance claim documents\"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_file: str, images_dir: str, processor: DonutProcessor, max_length: int = 128):\n",
    "        self.jsonl_file = jsonl_file\n",
    "        self.images_dir = images_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load your dataset annotations\n",
    "        self.annotations = self.load_annotations()\n",
    "        \n",
    "    def load_annotations(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load annotations from JSONL file\"\"\"\n",
    "        annotations = []\n",
    "        \n",
    "        # Load from JSONL file\n",
    "        if os.path.exists(self.jsonl_file):\n",
    "            with open(self.jsonl_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():  # Skip empty lines\n",
    "                        data = json.loads(line)\n",
    "                        \n",
    "                        # Build full image path\n",
    "                        image_filename = data.get(\"image_path\", data.get(\"image\", data.get(\"file_name\")))\n",
    "                        full_image_path = os.path.join(self.images_dir, image_filename)\n",
    "                        \n",
    "                        # Handle different possible formats for ground truth\n",
    "                        ground_truth = data.get(\"ground_truth\", data.get(\"annotation\", data.get(\"labels\")))\n",
    "                        \n",
    "                        # Convert ground truth to string if it's a dict\n",
    "                        if isinstance(ground_truth, dict):\n",
    "                            ground_truth = json.dumps(ground_truth)\n",
    "                        \n",
    "                        annotations.append({\n",
    "                            \"image_path\": full_image_path,\n",
    "                            \"ground_truth\": ground_truth\n",
    "                        })\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"JSONL file not found at {self.jsonl_file}\")\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(annotation[\"image_path\"]).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {annotation['image_path']}: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Create task prompt for insurance claims\n",
    "        task_prompt = \"<s_docvqa><s_question>Extract key information from this insurance claim document</s_question><s_answer>\"\n",
    "        \n",
    "        # Prepare target (ground truth)\n",
    "        target_text = annotation[\"ground_truth\"] + self.processor.tokenizer.eos_token\n",
    "        \n",
    "        # Process image only\n",
    "        image_encoding = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create decoder input ids (task prompt)\n",
    "        decoder_input_ids = self.processor.tokenizer(\n",
    "            task_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        # Create labels (task prompt + target)\n",
    "        full_target = task_prompt + target_text\n",
    "        labels = self.processor.tokenizer(\n",
    "            full_target,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image_encoding[\"pixel_values\"].squeeze(0),\n",
    "            \"decoder_input_ids\": decoder_input_ids[\"input_ids\"].squeeze(0),\n",
    "            \"labels\": labels[\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "class DonutTrainer(Trainer):\n",
    "    \"\"\"Custom trainer for Donut model with proper loss computation\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Custom loss computation with detailed debugging\"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Debug prints (remove in production)\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        \n",
    "        # Ensure we have valid labels\n",
    "        if labels is None:\n",
    "            return outputs.loss if return_outputs else None\n",
    "        \n",
    "        # For causal language modeling, shift labels and logits\n",
    "        if labels.dim() == 2:  # [batch_size, seq_len]\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            print(f\"Shift logits shape: {shift_logits.shape}\")\n",
    "            print(f\"Shift labels shape: {shift_labels.shape}\")\n",
    "            \n",
    "            # Flatten for loss computation\n",
    "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            \n",
    "            print(f\"Flattened logits shape: {shift_logits.shape}\")\n",
    "            print(f\"Flattened labels shape: {shift_labels.shape}\")\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "        else:\n",
    "            # Fallback to standard loss computation\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def prepare_model_and_processor(model_name: str = \"naver-clova-ix/donut-base-finetuned-docvqa\"):\n",
    "    \"\"\"Load pre-trained model and processor\"\"\"\n",
    "    \n",
    "    processor = DonutProcessor.from_pretrained(model_name)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Force model to CPU\n",
    "    #model = model.to('cpu')\n",
    "    model.to('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Configure model for fine-tuning\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def fine_tune_donut(\n",
    "    jsonl_file: str,\n",
    "    images_dir: str,\n",
    "    output_dir: str,\n",
    "    val_split: float = 0.2,\n",
    "    num_epochs: int = 3,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 5e-5\n",
    "):\n",
    "    \"\"\"Main fine-tuning function\"\"\"\n",
    "    \n",
    "    # Load model and processor\n",
    "    model, processor = prepare_model_and_processor()\n",
    "    \n",
    "    # Create full dataset first\n",
    "    full_dataset = InsuranceClaimDataset(jsonl_file, images_dir, processor)\n",
    "    \n",
    "    logger.info(f\"Total dataset size: {len(full_dataset)}\")\n",
    "    \n",
    "    # Handle small datasets\n",
    "    if len(full_dataset) < 5:\n",
    "        logger.warning(f\"Very small dataset ({len(full_dataset)} samples)! This may not be sufficient for meaningful training.\")\n",
    "        logger.warning(\"Consider:\")\n",
    "        logger.warning(\"1. Adding more training data\")\n",
    "        logger.warning(\"2. Using data augmentation\")\n",
    "        logger.warning(\"3. Setting val_split=0 to use all data for training\")\n",
    "        \n",
    "        # Force no validation split for very small datasets\n",
    "        if len(full_dataset) <= 3:\n",
    "            val_split = 0\n",
    "            logger.info(\"Forcing val_split=0 due to extremely small dataset\")\n",
    "    \n",
    "    # Split into train and validation\n",
    "    if val_split > 0 and len(full_dataset) > 3:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(full_dataset)),\n",
    "            test_size=val_split,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "        logger.info(f\"Validation samples: {len(val_dataset)}\")\n",
    "    else:\n",
    "        train_dataset = full_dataset\n",
    "        val_dataset = None\n",
    "        logger.info(f\"Training samples: {len(train_dataset)} (no validation split)\")\n",
    "    \n",
    "    # Adjust batch size for small datasets\n",
    "    actual_batch_size = min(batch_size, len(train_dataset))\n",
    "    if actual_batch_size < batch_size:\n",
    "        logger.warning(f\"Reducing batch size from {batch_size} to {actual_batch_size} due to small dataset\")\n",
    "    \n",
    "    # Training arguments - CPU optimized\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=actual_batch_size,\n",
    "        per_device_eval_batch_size=actual_batch_size,\n",
    "        warmup_steps=min(100, len(train_dataset) // actual_batch_size),  # Adjust warmup for small datasets\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=max(1, len(train_dataset) // actual_batch_size),  # Log every step for small datasets\n",
    "        save_strategy=\"epoch\",  # Save every epoch instead of steps for small datasets\n",
    "        eval_strategy=\"epoch\" if val_dataset is not None else \"no\",\n",
    "        load_best_model_at_end=val_dataset is not None,\n",
    "        dataloader_pin_memory=False,\n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        report_to=None,\n",
    "        no_cuda=False,  # Force CPU usage\n",
    "        fp16=False,  # Disable mixed precision for CPU\n",
    "        dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "        gradient_accumulation_steps=max(1, 4 // actual_batch_size),  # Simulate larger batch sizes\n",
    "        save_total_limit=2,  # Keep only 2 checkpoints to save space\n",
    "    )\n",
    "    \n",
    "    # Add validation settings if we have validation data\n",
    "    if val_dataset is not None:\n",
    "        training_args.metric_for_best_model = \"eval_loss\"\n",
    "        training_args.greater_is_better = False\n",
    "    \n",
    "    # Create data collator with proper max_length\n",
    "    data_collator = DataCollatorForDonut(\n",
    "        tokenizer=processor.tokenizer,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Use the custom trainer with improved loss computation\n",
    "    trainer = DonutTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Debug: Print a sample from the dataset\n",
    "    if len(train_dataset) > 0:\n",
    "        sample = train_dataset[0]\n",
    "        print(f\"Sample pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "        print(f\"Sample decoder_input_ids shape: {sample['decoder_input_ids'].shape}\")\n",
    "        print(f\"Sample labels shape: {sample['labels'].shape}\")\n",
    "    \n",
    "    # Start training\n",
    "    logger.info(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    logger.info(f\"Fine-tuning completed. Model saved to {output_dir}\")\n",
    "    \n",
    "    # Additional recommendations for small datasets\n",
    "    if len(full_dataset) < 10:\n",
    "        logger.info(\"RECOMMENDATIONS for small dataset:\")\n",
    "        logger.info(\"1. Consider data augmentation (rotation, scaling, noise)\")\n",
    "        logger.info(\"2. Use a pre-trained model that's already good at similar tasks\")\n",
    "        logger.info(\"3. Try few-shot learning approaches\")\n",
    "        logger.info(\"4. Collect more training data if possible\")\n",
    "\n",
    "def inference_example(model_path: str, image_path: str):\n",
    "    \"\"\"Example inference function - CPU only\"\"\"\n",
    "    \n",
    "    # Load fine-tuned model\n",
    "    processor = DonutProcessor.from_pretrained(model_path)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "    \n",
    "    # Force model to CPU and set to evaluation mode\n",
    "    #model = model.to('cpu')\n",
    "    device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Create task prompt\n",
    "    task_prompt = \"<s_docvqa><s_question>Extract key information from this insurance claim document</s_question><s_answer>\"\n",
    "    \n",
    "    # Process input - ensure tensors are on CPU\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    \n",
    "    # Extract the answer part\n",
    "    if \"<s_answer>\" in sequence:\n",
    "        sequence = sequence.split(\"<s_answer>\")[1]\n",
    "    \n",
    "    return sequence.strip()\n",
    "\n",
    "    # Example usage - CPU training with small dataset handling\n",
    "    if __name__ == \"__main__\":\n",
    "        # Configuration for CPU training\n",
    "        JSONL_FILE = \"Dataset/metadata.jsonl\"\n",
    "        IMAGES_DIR = \"Dataset/images/\"\n",
    "        OUTPUT_DIR = \"./donut-insurance-finetuned\"\n",
    "\n",
    "        print(\"Starting CPU-only training...\")\n",
    "        print(\"Warning: Training on CPU will be significantly slower than GPU!\")\n",
    "        print(\"Note: Small datasets (< 10 samples) may not provide meaningful results.\")\n",
    "        \n",
    "        # Fine-tune the model with CPU-optimized settings for small datasets\n",
    "        fine_tune_donut(\n",
    "            jsonl_file=JSONL_FILE,\n",
    "            images_dir=IMAGES_DIR,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            val_split=0.0,  # No validation split for small datasets\n",
    "            num_epochs=10,  # More epochs might help with small datasets\n",
    "            batch_size=1,   # Small batch size for CPU and small dataset\n",
    "            learning_rate=1e-5  # Lower learning rate for stability\n",
    "        )\n",
    "        \n",
    "        # Example inference\n",
    "        # result = inference_example(OUTPUT_DIR, \"path/to/test/image.jpg\")\n",
    "        # print(f\"Extracted information: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afd177b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CPU-only training...\n",
      "Warning: Training on CPU will be significantly slower than GPU!\n",
      "Consider using smaller batch sizes and fewer epochs for CPU training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total dataset size: 8\n",
      "INFO:__main__:Training samples: 6\n",
      "INFO:__main__:Validation samples: 2\n",
      "/tmp/ipykernel_141914/2509915925.py:333: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DonutTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DonutTrainer(\n",
      "INFO:__main__:Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample pixel_values shape: torch.Size([3, 2560, 1920])\n",
      "Sample decoder_input_ids shape: torch.Size([13])\n",
      "Sample labels shape: torch.Size([128])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 02:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>20.391476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>20.391476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n",
      "Logits shape: torch.Size([1, 128, 57532])\n",
      "Labels shape: torch.Size([1, 128])\n",
      "Shift logits shape: torch.Size([1, 127, 57532])\n",
      "Shift labels shape: torch.Size([1, 127])\n",
      "Flattened logits shape: torch.Size([127, 57532])\n",
      "Flattened labels shape: torch.Size([127])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['decoder.lm_head.weight'].\n",
      "INFO:__main__:Fine-tuning completed. Model saved to ./donut-insurance-finetuned\n",
      "INFO:__main__:RECOMMENDATIONS for small dataset:\n",
      "INFO:__main__:1. Consider data augmentation (rotation, scaling, noise)\n",
      "INFO:__main__:2. Use a pre-trained model that's already good at similar tasks\n",
      "INFO:__main__:3. Try few-shot learning approaches\n",
      "INFO:__main__:4. Collect more training data if possible\n"
     ]
    }
   ],
   "source": [
    "JSONL_FILE = \"Dataset/metadata.jsonl\"\n",
    "IMAGES_DIR = \"Dataset/images/\"\n",
    "OUTPUT_DIR = \"./donut-insurance-finetuned\"\n",
    "\n",
    "print(\"Starting CPU-only training...\")\n",
    "print(\"Warning: Training on CPU will be significantly slower than GPU!\")\n",
    "print(\"Consider using smaller batch sizes and fewer epochs for CPU training.\")\n",
    "\n",
    "# Fine-tune the model with CPU-optimized settings\n",
    "fine_tune_donut(\n",
    "    jsonl_file=JSONL_FILE,\n",
    "    images_dir=IMAGES_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    val_split=0.2,  # 20% for validation, set to 0 for no validation\n",
    "    num_epochs=2,   # Reduced epochs for CPU training\n",
    "    batch_size=1,   # Smaller batch size for CPU\n",
    "    learning_rate=5e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e81109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted information: scanning and measurement instructions</s_answer>\n"
     ]
    }
   ],
   "source": [
    "result = inference_example(OUTPUT_DIR, \"Dataset/images/test1.png\")\n",
    "print(f\"Extracted information: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb7c0aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./donut-insurance-finetuned'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7898467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c66b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc158bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "donut_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
